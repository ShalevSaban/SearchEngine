# ğŸ” Mini-Google â€“ Distributed Search Engine with Kafka & DFS Crawling

This project is a simplified, distributed search engine â€“ similar in concept to Google â€“ built using depth-first crawling, batch processing, and modern big-data tools like **Kafka** and **Elasticsearch**.

---

## ğŸ“Œ Overview

Mini-Google starts from a given URL, crawls the web recursively using a **DFS-based strategy**, extracts all the links from each page, and indexes the content.

Each crawled page is processed asynchronously using **Kafka**, and indexed for search using **Elasticsearch**. The architecture supports scaling by breaking the crawling and indexing into parallel, isolated jobs using **MapReduce-like batching**.

---

## âš™ï¸ Technologies Used

| Technology       | Purpose |
|------------------|---------|
| ğŸ§­ DFS Crawling   | Recursively visit web pages and extract links |
| ğŸ§µ Kafka          | Message queue for decoupled crawling & indexing |
| ğŸ—ƒï¸ Map-Reduce     | Split crawling/indexing into distributed jobs |
| ğŸ“¥ Read / Write   | Persist raw HTML and indexable data |
| ğŸ” Elasticsearch  | Index and search crawled pages |
| â˜ï¸ Deployment     | Can be deployed across multiple containers/nodes |

---

## ğŸ” System Flow

1. **Start from a seed URL** provided by the user.
2. **Crawl the page** using DFS:
   - Fetch HTML
   - Extract `<a href="">` links
   - Send each discovered URL to Kafka
3. **Kafka consumers** handle crawling jobs in parallel:
   - Fetch and clean the content
   - Extract keywords and metadata
4. **Processed content** is written into **Elasticsearch** as documents.
5. Users can then **search** any keyword via REST API or UI â†’ results are fetched from the Elasticsearch index.

---

## ğŸ§ª Example Flow

**User provides seed URL:**
```
https://example.com
```

**What happens:**
- Crawler visits the page and finds 5 links.
- Each link is added as a message in Kafka topic `crawl-requests`.
- Multiple consumers process those links concurrently.
- Cleaned page data is transformed and stored in Elasticsearch.

**Search request:**
```
GET /api/search?query=machine+learning
```

**Search response (from Elasticsearch):**
```json
[
  {
    "title": "Introduction to Machine Learning",
    "url": "https://example.com/ml",
    "snippet": "Machine learning is a field of AI that focuses on..."
  },
  ...
]
```

---

## ğŸ—ï¸ Architecture

- **Producer (Crawler)** â†’ reads pages, extracts links â†’ sends URLs to Kafka.
- **Kafka Topic (`crawl-requests`)** â†’ holds URLs waiting to be processed.
- **Consumers** â†’ fetch URLs from Kafka, download + parse HTML, clean content.
- **Elasticsearch** â†’ indexes cleaned data with metadata and keywords.
- **REST API / UI** â†’ exposes search functionality over indexed content.

---

## ğŸš€ Deployment

This system is designed for distributed environments:

- Each crawler, processor, and indexer can run in its own container or VM.
- Kafka ensures fault-tolerant communication between stages.
- Elasticsearch handles horizontal scaling of indexing and querying.

---

## ğŸ§  Future Enhancements

- [ ] Add page rank scoring
- [ ] Detect and avoid duplicate content
- [ ] Handle robots.txt and rate limiting
- [ ] Visual frontend for query results
- [ ] Full-text indexing with NLP enrichment

---

## ğŸ”— GitHub Repository

[ğŸ“‚ ShalevSaban/SearchEngine](https://github.com/ShalevSaban/SearchEngine)
